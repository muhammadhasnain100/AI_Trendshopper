{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T13:29:17.168747896Z",
     "start_time": "2025-04-23T13:29:15.291857755Z"
    }
   },
   "id": "7cc13a83ef0360fa"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T05:31:02.167137200Z",
     "start_time": "2025-04-24T05:30:56.058540573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped collection: marketing_agent\n",
      "Dropped collection: order\n",
      "Dropped collection: ride_offers\n",
      "Dropped collection: cart\n",
      "Dropped collection: posts\n",
      "Dropped collection: users\n",
      "Dropped collection: products\n",
      "Dropped collection: chats\n",
      "Dropped collection: images\n",
      "Dropped collection: shops\n",
      "Dropped collection: ride_requests\n",
      "Dropped collection: vehicles\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "from config import settings\n",
    "\n",
    "client = AsyncIOMotorClient(settings.MONGO_DETAILS, serverSelectionTimeoutMS=5000)\n",
    "db = client.user_db\n",
    "\n",
    "async def drop_all_collections():\n",
    "    # List all collection names in the database\n",
    "    colls = await db.list_collection_names()\n",
    "    if not colls:\n",
    "        print(\"No collections to drop.\")\n",
    "        return\n",
    "\n",
    "    # Drop each one\n",
    "    for coll_name in colls:\n",
    "        await db.drop_collection(coll_name)\n",
    "        print(f\"Dropped collection: {coll_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await drop_all_collections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistani salwar kameez designs for weddings offer a variety of styles, colors, and embellishments. Here's a breakdown of some popular trends:\n",
      "\n",
      "**1. Design Styles:**\n",
      "\n",
      "*   **Sharara Suits:** Shararas are making a comeback, featuring wide-legged pants that flare out from the knee, paired with a short tunic. Some designs have the salwar fitted above the knee, adorned with zari and sequin work.\n",
      "*   **Floor-Length Salwar Kameez:** These are a popular choice for wedding attire, resembling Western gowns with an ethnic touch. They often feature elaborate embroidery, stonework, and heavy ornamentation.\n",
      "*   **Anarkali Suits:** Anarkalis remain a timeless and popular choice, known for giving a regal look. They have a beautiful neckline, a flared bottom, and flowing pleats.\n",
      "*   **Pakistani Sharara Suit:** A stylish variation with a long, flowing tunic and wide-legged pants resembling a skirt, fitted at the waist and flared from the knee.\n",
      "\n",
      "**2. Embellishments and Techniques:**\n",
      "\n",
      "*   **Embroidery:** Intricate embroidery is a key element, with popular techniques including Zardozi, Phulkari, Chikankari, and Resham work.\n",
      "*   **Block Printing:** A traditional technique using hand-carved wooden blocks to stamp patterns onto the fabric.\n",
      "*   **Digital Printing:** Modern technique for creating intricate, high-resolution patterns with vivid colors.\n",
      "*   **AppliquÃ© and Patchwork:** Attaching fabric pieces to the base fabric to create unique designs.\n",
      "*   **Mirror Work**: Intricate mirror work is used to adorn the piece.\n",
      "\n",
      "**3. Fabrics:**\n",
      "\n",
      "*   **Georgette:** Lightweight and flowy, suitable for festive and formal occasions.\n",
      "*   **Chiffon:** Sheer and lightweight, creating a graceful look.\n",
      "*   **Silk:** A popular choice for wedding salwar kameez.\n",
      "*   **Lawn:** A lightweight fabric woven using cotton and dyed in bright colors\n"
     ]
    }
   ],
   "source": [
    "print(refined_prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:39:43.790325588Z",
     "start_time": "2025-03-22T19:39:43.719834867Z"
    }
   },
   "id": "effb74d61dac8e0e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Use the refined prompt to generate an image with attributes emphasizing 'professional' and 'best'\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\n",
    "        f\"extract the one prompt detail add for only one dress for dress {dress_type}.\"\n",
    "        f\"context: {refined_prompt}\"\n",
    "    ],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=[\"Text\", \"Image\"]\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:39:45.194179524Z",
     "start_time": "2025-03-22T19:39:43.732789889Z"
    }
   },
   "id": "170ce4439f1fe497"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Some designs have the salwar fitted above the knee, adorned with zari and sequin work.\" This detail is specifically added for the **Sharara Suits** dress style within the Pakistani salwar kameez context.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:39:45.250069616Z",
     "start_time": "2025-03-22T19:39:45.198717226Z"
    }
   },
   "id": "3fdde8bfba3283b3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to professional_best_design.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = (\n",
    "            f\"Generate an eye-catching advertising poster for the shop named laracloth. \"\n",
    "            f\"Include the shop's tagline we provide best designs, and contact information laracloth12@gmail.com or reference our website.dont need to add any logo image just show theme we are selling cloth product\"\n",
    "            \"The style should be modern, vibrant, and persuasive.\"\n",
    "        )\n",
    "# Use the refined prompt to generate an image with attributes emphasizing 'professional' and 'best'\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=[\"Text\", \"Image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "def save_image(response, path):\n",
    "    # Loop through candidate parts and check for inline image data.\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        # Skip parts with text content.\n",
    "        if part.text is not None:\n",
    "            continue\n",
    "        elif part.inline_data is not None:\n",
    "            mime = part.inline_data.mime_type\n",
    "            data = part.inline_data.data\n",
    "            # If data is a base64 string, decode it.\n",
    "            if isinstance(data, str):\n",
    "                data = base64.b64decode(data)\n",
    "            image = PIL.Image.open(io.BytesIO(data))\n",
    "            image.save(path, format=\"PNG\")\n",
    "            print('Saved image to', path)\n",
    "\n",
    "# Save the generated image.\n",
    "save_image(response, 'professional_best_design.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:00:19.580126806Z",
     "start_time": "2025-04-23T15:00:08.825858106Z"
    }
   },
   "id": "85ecae6a2447a020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "378b90513b463b0b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to professional_best_design.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "product = PIL.Image.open('/home/hasnain/Downloads/IMG_4482.PNG')\n",
    "\n",
    "# Use the refined prompt to generate an image with attributes emphasizing 'professional' and 'best'\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=(['create a image of angry looking in nice and beautiful look standing in street', product]\n",
    "    ),\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=[\"Text\", \"Image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "def save_image(response, path):\n",
    "    # Loop through candidate parts and check for inline image data.\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        # Skip parts with text content.\n",
    "        if part.text is not None:\n",
    "            print(part.text)\n",
    "            continue\n",
    "        elif part.inline_data is not None:\n",
    "            mime = part.inline_data.mime_type\n",
    "            data = part.inline_data.data\n",
    "            # If data is a base64 string, decode it.\n",
    "            if isinstance(data, str):\n",
    "                data = base64.b64decode(data)\n",
    "            image = PIL.Image.open(io.BytesIO(data))\n",
    "            image.save(path, format=\"PNG\")\n",
    "            print('Saved image to', path)\n",
    "\n",
    "# Save the generated image.\n",
    "save_image(response, 'professional_best_design.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:46:44.106721497Z",
     "start_time": "2025-03-23T10:46:35.078186822Z"
    }
   },
   "id": "cf93eb472c22c77a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to professional_best_design.png\n"
     ]
    }
   ],
   "source": [
    "product = PIL.Image.open('/home/hasnain/Downloads/AirBrush_20241113141300.jpg')\n",
    "person = PIL.Image.open('/home/hasnain/Downloads/Models/Images/Boys/Muhammad_Hasnain/Snapinsta.app_329731501_119610914211714_4817348222872244364_n_1080.jpg')\n",
    "# Use the refined prompt to generate an image with attributes emphasizing 'professional' and 'best'\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=(['You will act like visual try on first image is about the product and second image about the person. first image show product and it should be wear by the person in the second image. If there is not person in second image then dont generate the image and generate a simple text. MAke sure that person in generated image should be same as it is provided in second image and dont need to change and replace and i want correct generated second person with wear product. If there is some illogical then dont generate image ', product, person]\n",
    "    ),\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=[\"Text\", \"Image\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "def save_image(response, path):\n",
    "    # Loop through candidate parts and check for inline image data.\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        # Skip parts with text content.\n",
    "        if part.text is not None:\n",
    "            print(part.text)\n",
    "            continue\n",
    "        elif part.inline_data is not None:\n",
    "            mime = part.inline_data.mime_type\n",
    "            data = part.inline_data.data\n",
    "            # If data is a base64 string, decode it.\n",
    "            if isinstance(data, str):\n",
    "                data = base64.b64decode(data)\n",
    "            image = PIL.Image.open(io.BytesIO(data))\n",
    "            image.save(path, format=\"PNG\")\n",
    "            print('Saved image to', path)\n",
    "\n",
    "# Save the generated image.\n",
    "save_image(response, 'professional_best_design.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-21T19:08:16.805383818Z",
     "start_time": "2025-03-21T19:08:06.000822711Z"
    }
   },
   "id": "4075679178fed79b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "def send_response_email(email_subject, email_text, receiving_email):\n",
    "    sender_email = 'hasnainnaseer987@gmail.com'\n",
    "    receiver_email = receiving_email\n",
    "    password = 'vnlh hafc sflj fwrp'\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    port = 587\n",
    "\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] = email_subject\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "\n",
    "    text = email_text\n",
    "\n",
    "    part1 = MIMEText(text, \"plain\")\n",
    "    message.attach(part1)\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiving_email, message.as_string())\n",
    "        server.quit()\n",
    "        return \"Successfully sent email\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-21T18:54:57.820983986Z",
     "start_time": "2025-03-21T18:54:57.776222298Z"
    }
   },
   "id": "3ba7816aa3e7a0d8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'Successfully sent email'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_response_email('testing', 'hi how are you?', 'fa21-bai-058@cuiatk.edu.pk')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-21T18:55:45.995976878Z",
     "start_time": "2025-03-21T18:55:42.281400713Z"
    }
   },
   "id": "419ce7412ec34d7e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1 documents from the users collection.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "from config import settings\n",
    "import asyncio\n",
    "\n",
    "# Allow nesting of the event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def delete_all_users():\n",
    "    # Create an AsyncIOMotorClient\n",
    "    client_db = AsyncIOMotorClient(settings.MONGO_DETAILS, serverSelectionTimeoutMS=5000)\n",
    "    database = client_db.user_db\n",
    "    user_collection = database.users\n",
    "    \n",
    "    # Delete all documents in the users collection\n",
    "    result = await user_collection.delete_many({})\n",
    "    \n",
    "    # Print the number of deleted documents\n",
    "    print(f'Deleted {result.deleted_count} documents from the users collection.')\n",
    "\n",
    "# Run the delete_all_users function using asyncio.run\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(delete_all_users())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T09:02:12.421881168Z",
     "start_time": "2025-03-22T09:02:12.403128322Z"
    }
   },
   "id": "c8512ca9c39d0af4"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-22-2025\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "joining_date = datetime.utcnow().strftime(\"%m-%d-%Y\")\n",
    "print(joining_date)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T09:01:38.689157081Z",
     "start_time": "2025-03-22T09:01:38.630320410Z"
    }
   },
   "id": "fb2564fb895a543"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 153\u001B[0m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mprint\u001B[39m(campaign_results)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/asyncio/runners.py:33\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    asyncio.run(main())\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m coroutines\u001B[38;5;241m.\u001B[39miscoroutine(main):\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma coroutine was expected, got \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(main))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import smtplib\n",
    "from datetime import datetime\n",
    "from aitrendshopper.database import shop_collection, product_collection, user_collection, marketing_collection\n",
    "from aitrendshopper.config import settings\n",
    "from bson import ObjectId\n",
    "\n",
    "class MarketingBot:\n",
    "    def __init__(self):\n",
    "        self.sending_email = 'hasnainnaseer987@gmail.com'\n",
    "        self.password = 'vnlh hafc sflj fwrp'  # Replace with a secure method of retrieving credentials.\n",
    "        self.model_id = 'gemini-2.0-flash'\n",
    "        self.image_model_id = \"gemini-2.0-flash-exp\"\n",
    "        self.client = settings.client\n",
    "    \n",
    "    def send_email(self, email_subject, email_text, receiver_email, receiver_name=\"Valued Customer\"):\n",
    "        \"\"\"\n",
    "        Send a personalized marketing email via SMTP.\n",
    "        Returns a string indicating success or an error message.\n",
    "        \"\"\"\n",
    "        sender_email = self.sending_email\n",
    "        smtp_server = \"smtp.gmail.com\"\n",
    "        port = 587\n",
    "\n",
    "        # Use a professional greeting in the email text.\n",
    "        professional_email_text = f\"Dear {receiver_name},\\n\\n\" + email_text\n",
    "\n",
    "        message = MIMEMultipart(\"alternative\")\n",
    "        message[\"Subject\"] = email_subject\n",
    "        message[\"From\"] = sender_email\n",
    "        message[\"To\"] = receiver_email\n",
    "\n",
    "        # Attach the plain text part.\n",
    "        part1 = MIMEText(professional_email_text, \"plain\")\n",
    "        message.attach(part1)\n",
    "\n",
    "        try:\n",
    "            server = smtplib.SMTP(smtp_server, port)\n",
    "            server.starttls()\n",
    "            server.login(sender_email, self.password)\n",
    "            server.sendmail(sender_email, receiver_email, message.as_string())\n",
    "            server.quit()\n",
    "            return \"Successful\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def blog_post(self, context):\n",
    "        \"\"\"\n",
    "        Generate and return a captivating blog post using the language model.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are an expert content strategist and marketing professional.\n",
    "Generate a captivating blog post that highlights the unique features and benefits \n",
    "of our latest product using persuasive and professional language.\n",
    "Include an engaging headline, clear sections, and a strong call-to-action.\n",
    "Write just the blog post text.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Template:\n",
    "[shop_name] â€“ [tagline]\n",
    "![Shop Banner]([shop_banner URL])\n",
    "\n",
    "About Us:\n",
    "[description]\n",
    "Location: [address]\n",
    "Contact:\n",
    "Phone: [contact_number]\n",
    "Email: [contact_email]\n",
    "\n",
    "Featured Product: [product_name]\n",
    "![Product Image]([product_image URL])\n",
    "\n",
    "Product Details:\n",
    "[description]\n",
    "Price: $[price]\n",
    "Available Quantity: [quantity]\n",
    "\n",
    "Why Shop With Us?\n",
    "Experience quality and exceptional service at [shop_name]. Our featured product, [product_name], is designed to deliver outstanding value and performance.\n",
    "\n",
    "Visit Us Today:\n",
    "Discover more about our products and special offers by visiting our store or contacting us directly.\n",
    "\"\"\"\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model_id,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    async def marketing_strategy_prompt(self, context):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive and professional marketing strategy in JSON format.\n",
    "        The output includes target audience analysis, social media campaign ideas, email marketing plans,\n",
    "        promotional poster designs, budget planning, campaign duration, and performance metrics.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Generate a comprehensive and professional marketing strategy for our shop. \n",
    "Include a detailed target audience analysis, creative social media campaign ideas, \n",
    "effective email marketing plans, and attractive promotional poster design suggestions. \n",
    "Also provide a proposed campaign budget, recommended campaign duration, and key performance metrics for success. \n",
    "Use persuasive language and actionable steps. \n",
    "Context: {context}\n",
    "Generate the answer according to provided json format\n",
    "Output JSON Format (Make sure the output is valid JSON):\n",
    "\n",
    "{{\n",
    "  \"target_audience\": \"Provide two descriptive words that best describe the target audience.\",\n",
    "  \"poster_design\": \"Provide creative poster design ideas in a string of approximately 20 words.\",\n",
    "  \"gender\": \"Specify the target gender: 'male', 'female', or 'both'.\",\n",
    "  \"age\": \"Specify the ideal age range for the product (e.g., '18-35').\",\n",
    "  \"social_media\": \"Provide social media marketing ideas for platforms such as TikTok, Instagram, YouTube, and Facebook in a string of around 20 words.\",\n",
    "  \"email_marketing\": \"Describe detailed email marketing strategies in one concise string. 20 words\",\n",
    "  \"campaign_duration\": \"Specify the recommended duration for the campaign (e.g., '3 months').\"\n",
    "}}\n",
    "\"\"\"\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model_id,\n",
    "            contents=prompt,\n",
    "            config=self.client.types.GenerateContentConfig(response_mime_type=\"application/json\")\n",
    "        )\n",
    "        return json.loads(response.text)\n",
    "    \n",
    "    async def start_campaign(self, product_id, shop_id):\n",
    "        # Fetch product and shop details.\n",
    "        product = await product_collection.find_one({\"_id\": product_id})\n",
    "        shop = await shop_collection.find_one({\"_id\": shop_id})\n",
    "        if not product or not shop:\n",
    "            raise Exception(\"Invalid product or shop ID provided.\")\n",
    "        \n",
    "        # Compose the email message.\n",
    "        email_subject = f\"Introducing Our Latest Product: {product.get('product_name', '')}\"\n",
    "        email_body = (\n",
    "            f\"We are delighted to announce the launch of our new product, '{product.get('product_name', '')}', \"\n",
    "            f\"available now at '{shop.get('shop_name', '')}'.\\n\\n\"\n",
    "            f\"Product Overview:\\n{product.get('description', '')}\\n\\n\"\n",
    "            f\"Price: ${product.get('price', '')}\\n\\n\"\n",
    "            \"We invite you to explore this exclusive offering and experience the quality and innovation \"\n",
    "            \"that define our brand.\\n\\n\"\n",
    "            \"Best regards,\\n\"\n",
    "            f\"The {shop.get('shop_name', '')} Team\"\n",
    "        )\n",
    "        \n",
    "        # Retrieve all user documents from the database.\n",
    "        users = await user_collection.find({}).to_list(length=1000)\n",
    "        \n",
    "        # Generate marketing materials.\n",
    "        blog = self.blog_post(f'Product details: {product}\\nShop details: {shop}')\n",
    "        strategy = await self.marketing_strategy_prompt(f'Product details: {product}\\nShop details: {shop}')\n",
    "        \n",
    "        # Prepare campaign data with enriched analysis values.\n",
    "        campaign_data = {\n",
    "            \"_id\": str(ObjectId()),\n",
    "            \"product_id\": product_id,\n",
    "            \"shop_id\": shop_id,\n",
    "            \"target_mails\": len(users),\n",
    "            \"successfully_send_mails\": 0,\n",
    "            \"blog_post\": blog,\n",
    "            \"marketing_strategy\": strategy,\n",
    "            \"created_at\": datetime.utcnow(),\n",
    "        }\n",
    "        # Insert campaign details into the marketing collection.\n",
    "        await marketing_collection.insert_one(campaign_data)\n",
    "        \n",
    "        for user_doc in users:\n",
    "            email = user_doc.get(\"email\")\n",
    "            # Use the user's full name if available.\n",
    "            receiver_name = user_doc.get(\"name\", \"Valued Customer\")\n",
    "            if email:\n",
    "                result = self.send_email(email_subject, email_body, email, receiver_name)\n",
    "                if \"successful\" in result.lower():\n",
    "                    # Update the campaign document by incrementing the count.\n",
    "                    await marketing_collection.update_one(\n",
    "                        {\"product_id\": product_id},\n",
    "                        {\"$inc\": {\"successfully_send_mails\": 1}}\n",
    "                    )\n",
    "                else:\n",
    "                    # Log or handle the failed email send attempt here.\n",
    "                    pass\n",
    "    \n",
    "    async def get_result(self, product_id):\n",
    "        \"\"\"\n",
    "        Retrieve products based on a given product_id.\n",
    "        \"\"\"\n",
    "        result = await product_collection.find({\"product_id\": product_id}).to_list(length=1000)\n",
    "        return result\n",
    "        \n",
    "# Example usage:\n",
    "async def main():\n",
    "    bot = MarketingBot()\n",
    "    \n",
    "    # Replace with valid product and shop IDs from your database.\n",
    "    product_id = \"product_id_here\"\n",
    "    shop_id = \"shop_id_here\"\n",
    "    \n",
    "    await bot.start_campaign(product_id, shop_id)\n",
    "    print(\"\\nCampaign initiated successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:38:04.674251899Z",
     "start_time": "2025-03-22T19:38:01.628296958Z"
    }
   },
   "id": "a2a1f35ca50a94f3"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from google import genai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T18:51:38.530937320Z",
     "start_time": "2025-04-12T18:51:34.946147638Z"
    }
   },
   "id": "7864ca1689728802"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "Okay! If you need anything at all, just let me know. I'm here to answer questions, brainstorm ideas, write different kinds of content, translate languages, and much more. Just tell me what's on your mind!\n",
      "Is there anything I can do for you? Perhaps you accidentally hit send without typing anything? Let me know if you need assistance with anything!\n",
      "Got it! I understand you're just saying hi again.  ðŸ‘‹  Is there anything you'd like to chat about, or any tasks you'd like me to help with?  If not, that's perfectly fine too!  Just let me know.\n",
      "Okay, just checking in!  Let me know if you need anything at all. I'm here and ready to assist.  Have a great day!\n",
      "Alright! Just signaling that you're still there, I presume? ðŸ˜Š Feel free to use me for anything, big or small. Otherwise, have a good one!\n",
      "Okay, I will stop responding unless you ask me to do something. Just let me know when you need me!\n",
      "Understood. I will remain silent until you need me. Just let me know!\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCancelledError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     18\u001B[0m                     \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mtext, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m main()\n",
      "Cell \u001B[0;32mIn[3], line 16\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m session\u001B[38;5;241m.\u001B[39msend_client_content(\n\u001B[1;32m     13\u001B[0m     turns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparts\u001B[39m\u001B[38;5;124m\"\u001B[39m: [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: message}]}, turn_complete\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     14\u001B[0m )\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m session\u001B[38;5;241m.\u001B[39mreceive():\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mtext, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/fastApiProject/venv/lib/python3.10/site-packages/google/genai/live.py:413\u001B[0m, in \u001B[0;36mAsyncSession.receive\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Receive model responses from the server.\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \n\u001B[1;32m    393\u001B[0m \u001B[38;5;124;03mThe method will yield the model responses from the server. The returned\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;124;03m      print(message)\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    412\u001B[0m \u001B[38;5;66;03m# TODO(b/365983264) Handle intermittent issues for the user.\u001B[39;00m\n\u001B[0;32m--> 413\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m result \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_receive():\n\u001B[1;32m    414\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mserver_content \u001B[38;5;129;01mand\u001B[39;00m result\u001B[38;5;241m.\u001B[39mserver_content\u001B[38;5;241m.\u001B[39mturn_complete:\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/fastApiProject/venv/lib/python3.10/site-packages/google/genai/live.py:494\u001B[0m, in \u001B[0;36mAsyncSession._receive\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    492\u001B[0m parameter_model \u001B[38;5;241m=\u001B[39m types\u001B[38;5;241m.\u001B[39mLiveServerMessage()\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 494\u001B[0m   raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ws\u001B[38;5;241m.\u001B[39mrecv(decode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    496\u001B[0m   raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ws\u001B[38;5;241m.\u001B[39mrecv()  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/fastApiProject/venv/lib/python3.10/site-packages/websockets/asyncio/connection.py:294\u001B[0m, in \u001B[0;36mConnection.recv\u001B[0;34m(self, decode)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;124;03mReceive the next message.\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m \n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecv_messages\u001B[38;5;241m.\u001B[39mget(decode)\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m:\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/fastApiProject/venv/lib/python3.10/site-packages/websockets/asyncio/messages.py:150\u001B[0m, in \u001B[0;36mAssembler.get\u001B[0;34m(self, decode)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;66;03m# Locking with get_in_progress prevents concurrent execution\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# until get() fetches a complete message or is cancelled.\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;66;03m# First frame\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m     frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframes\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclosed)\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaybe_resume()\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m frame\u001B[38;5;241m.\u001B[39mopcode \u001B[38;5;129;01mis\u001B[39;00m OP_TEXT \u001B[38;5;129;01mor\u001B[39;00m frame\u001B[38;5;241m.\u001B[39mopcode \u001B[38;5;129;01mis\u001B[39;00m OP_BINARY\n",
      "File \u001B[0;32m~/PycharmProjects/fastApiProject/venv/lib/python3.10/site-packages/websockets/asyncio/messages.py:51\u001B[0m, in \u001B[0;36mSimpleQueue.get\u001B[0;34m(self, block)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_waiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloop\u001B[38;5;241m.\u001B[39mcreate_future()\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_waiter\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_waiter\u001B[38;5;241m.\u001B[39mcancel()\n",
      "\u001B[0;31mCancelledError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyA005hOPR4wXEqQXUfZzro8N_Q9umXCiHg\")\n",
    "model = \"gemini-2.0-flash-live-001\"\n",
    "\n",
    "config = {\"response_modalities\": [\"TEXT\"]}\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model, config=config) as session:\n",
    "        while True:\n",
    "            message = input(\"User> \")\n",
    "            if message.lower() == \"exit\":\n",
    "                break\n",
    "            await session.send_client_content(\n",
    "                turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n",
    "            )\n",
    "\n",
    "            async for response in session.receive():\n",
    "                if response.text is not None:\n",
    "                    print(response.text, end=\"\")\n",
    "\n",
    "await main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T18:52:32.164458957Z",
     "start_time": "2025-04-12T18:51:40.669509933Z"
    }
   },
   "id": "4e875c9afa8d6c8a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import wave\n",
    "from google import genai\n",
    "\n",
    "# Define async_enumerate to enumerate over an async iterator.\n",
    "async def async_enumerate(aiterable, start=0):\n",
    "    index = start\n",
    "    async for item in aiterable:\n",
    "        yield index, item\n",
    "        index += 1\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyA005hOPR4wXEqQXUfZzro8N_Q9umXCiHg\", http_options={'api_version': 'v1alpha'})\n",
    "model = \"gemini-2.0-flash-live-001\"\n",
    "\n",
    "config = {\"response_modalities\": [\"AUDIO\"]}\n",
    "\n",
    "async def main():\n",
    "    async with client.aio.live.connect(model=model, config=config) as session:\n",
    "        wf = wave.open(\"audio.wav\", \"wb\")\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(24000)\n",
    "\n",
    "        message = \"Hello? Gemini are you there?\"\n",
    "        await session.send_client_content(\n",
    "            turns={\"role\": \"user\", \"parts\": [{\"text\": message}]},\n",
    "            turn_complete=True\n",
    "        )\n",
    "\n",
    "        async for idx, response in async_enumerate(session.receive()):\n",
    "            if response.data is not None:\n",
    "                wf.writeframes(response.data)\n",
    "           \n",
    "        wf.close()\n",
    "\n",
    "await main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T19:19:05.320693206Z",
     "start_time": "2025-04-12T19:19:00.430099468Z"
    }
   },
   "id": "2e7f7130d00b6d9d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5911/3690316125.py:63: DeprecationWarning: The `session.send` method is deprecated and will be removed in a future version (not before Q3 2025).\n",
      "Please use one of the more specific methods: `send_client_content`, `send_realtime_input`, or `send_tool_response` instead.\n",
      "  await self.session.send(input=msg)\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loop was cancelled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import traceback\n",
    "import pyaudio\n",
    "from google import genai\n",
    "\n",
    "# For Python < 3.11, use fallback implementations for TaskGroup and ExceptionGroup.\n",
    "if sys.version_info < (3, 11, 0):\n",
    "    import taskgroup, exceptiongroup\n",
    "    asyncio.TaskGroup = taskgroup.TaskGroup\n",
    "    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup\n",
    "\n",
    "# Audio configuration parameters.\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "SEND_SAMPLE_RATE = 16000    # Microphone sample rate (16 kHz)\n",
    "RECEIVE_SAMPLE_RATE = 24000 # Output sample rate (24 kHz)\n",
    "CHUNK_SIZE = 1024           # Chunk size (should be a multiple of 320 bytes if required)\n",
    "\n",
    "# Gemini Live API model and configuration.\n",
    "MODEL = \"models/gemini-2.0-flash-live-001\"\n",
    "CONFIG = {\"response_modalities\": [\"AUDIO\"]}\n",
    "\n",
    "# Instantiate PyAudio and the Gemini client.\n",
    "pya = pyaudio.PyAudio()\n",
    "client = genai.Client(api_key=\"AIzaSyA005hOPR4wXEqQXUfZzro8N_Q9umXCiHg\", http_options={\"api_version\": \"v1beta\"})\n",
    "\n",
    "class AudioLoop:\n",
    "    def __init__(self):\n",
    "        # Queues for outbound (captured) and inbound (received) audio data.\n",
    "        self.out_queue = asyncio.Queue()\n",
    "        self.audio_in_queue = asyncio.Queue()\n",
    "        self.session = None\n",
    "        self.audio_stream = None\n",
    "\n",
    "    async def listen_audio(self):\n",
    "        \"\"\"\n",
    "        Capture audio from the microphone and place raw PCM chunks into the out_queue.\n",
    "        \"\"\"\n",
    "        mic_info = pya.get_default_input_device_info()\n",
    "        self.audio_stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=SEND_SAMPLE_RATE,\n",
    "            input=True,\n",
    "            input_device_index=mic_info[\"index\"],\n",
    "            frames_per_buffer=CHUNK_SIZE,\n",
    "        )\n",
    "        # Use kwargs to allow non-fatal overflows.\n",
    "        kwargs = {\"exception_on_overflow\": False} if __debug__ else {}\n",
    "        while True:\n",
    "            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)\n",
    "            # Each message is a dict with audio data and its MIME type.\n",
    "            await self.out_queue.put({\"data\": data, \"mime_type\": \"audio/pcm\"})\n",
    "\n",
    "    async def send_audio(self):\n",
    "        \"\"\"\n",
    "        Take audio chunks from the out_queue and send them via the Gemini session.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            msg = await self.out_queue.get()\n",
    "            await self.session.send(input=msg)\n",
    "\n",
    "    async def receive_audio(self):\n",
    "        \"\"\"\n",
    "        Receive audio from the Gemini session, then place raw PCM data from responses into audio_in_queue.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # 'session.receive()' returns an async generator.\n",
    "            turn = self.session.receive()\n",
    "            async for response in turn:\n",
    "                if response.data:\n",
    "                    self.audio_in_queue.put_nowait(response.data)\n",
    "                if response.text:\n",
    "                    print(response.text, end=\"\")\n",
    "            # Clear out any unread audio chunks if the turn completes.\n",
    "            while not self.audio_in_queue.empty():\n",
    "                self.audio_in_queue.get_nowait()\n",
    "\n",
    "    async def play_audio(self):\n",
    "        \"\"\"\n",
    "        Take PCM audio chunks from the incoming audio queue and play them through the speaker.\n",
    "        \"\"\"\n",
    "        output_stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=RECEIVE_SAMPLE_RATE,\n",
    "            output=True,\n",
    "        )\n",
    "        while True:\n",
    "            audio_chunk = await self.audio_in_queue.get()\n",
    "            await asyncio.to_thread(output_stream.write, audio_chunk)\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        Connect to the Gemini Live API and run the audio pipeline tasks concurrently.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with client.aio.live.connect(model=MODEL, config=CONFIG) as session, asyncio.TaskGroup() as tg:\n",
    "                self.session = session\n",
    "                # Start asynchronous tasks for capturing, sending, receiving, and playing audio.\n",
    "                tg.create_task(self.listen_audio())\n",
    "                tg.create_task(self.send_audio())\n",
    "                tg.create_task(self.receive_audio())\n",
    "                tg.create_task(self.play_audio())\n",
    "                # Here, we simply run indefinitely. In your notebook, you may stop the execution with an interrupt.\n",
    "                await asyncio.sleep(3600)\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"Audio loop was cancelled.\")\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(e)\n",
    "        finally:\n",
    "            if self.audio_stream:\n",
    "                self.audio_stream.close()\n",
    "\n",
    "# For running in a notebook or script.\n",
    "async def main():\n",
    "    audio_loop = AudioLoop()\n",
    "    await audio_loop.run()\n",
    "\n",
    "# If running as a script, use asyncio.run(main())\n",
    "await main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T10:37:08.054833536Z",
     "start_time": "2025-04-13T10:36:37.271781323Z"
    }
   },
   "id": "785af32234e61f5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11330/2387555930.py:63: DeprecationWarning: The `session.send` method is deprecated and will be removed in a future version (not before Q3 2025).\n",
      "Please use one of the more specific methods: `send_client_content`, `send_realtime_input`, or `send_tool_response` instead.\n",
      "  await self.session.send(input=msg)\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n",
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loop was cancelled.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import traceback\n",
    "import pyaudio\n",
    "from google import genai\n",
    "\n",
    "# For Python < 3.11, use fallback implementations for TaskGroup and ExceptionGroup.\n",
    "if sys.version_info < (3, 11, 0):\n",
    "    import taskgroup, exceptiongroup\n",
    "    asyncio.TaskGroup = taskgroup.TaskGroup\n",
    "    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup\n",
    "\n",
    "# Audio configuration parameters.\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "SEND_SAMPLE_RATE = 16000    # Microphone sample rate (16 kHz)\n",
    "RECEIVE_SAMPLE_RATE = 24000 # Output sample rate (24 kHz)\n",
    "CHUNK_SIZE = 1024           # Chunk size (should be a multiple of 320 bytes if required)\n",
    "\n",
    "# Gemini Live API model and configuration.\n",
    "MODEL = \"models/gemini-2.0-flash-live-001\"\n",
    "CONFIG = {\"response_modalities\": [\"AUDIO\"]}\n",
    "\n",
    "# Instantiate PyAudio and the Gemini client.\n",
    "pya = pyaudio.PyAudio()\n",
    "client = genai.Client(api_key='AIzaSyAAaqM_Vamz26ZlsI3sgkYvcMQcGcCI3To', http_options={\"api_version\": \"v1beta\"})\n",
    "\n",
    "class AudioLoop:\n",
    "    def __init__(self):\n",
    "        # Queues for outbound (captured) and inbound (received) audio data.\n",
    "        self.out_queue = asyncio.Queue()\n",
    "        self.audio_in_queue = asyncio.Queue()\n",
    "        self.session = None\n",
    "        self.audio_stream = None\n",
    "\n",
    "    async def listen_audio(self):\n",
    "        \"\"\"\n",
    "        Capture audio from the microphone and place raw PCM chunks into the out_queue.\n",
    "        \"\"\"\n",
    "        mic_info = pya.get_default_input_device_info()\n",
    "        self.audio_stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=SEND_SAMPLE_RATE,\n",
    "            input=True,\n",
    "            input_device_index=mic_info[\"index\"],\n",
    "            frames_per_buffer=CHUNK_SIZE,\n",
    "        )\n",
    "        # Use kwargs to allow non-fatal overflows.\n",
    "        kwargs = {\"exception_on_overflow\": False} if __debug__ else {}\n",
    "        while True:\n",
    "            data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)\n",
    "            # Each message is a dict with audio data and its MIME type.\n",
    "            await self.out_queue.put({\"data\": data, \"mime_type\": \"audio/pcm\"})\n",
    "\n",
    "    async def send_audio(self):\n",
    "        \"\"\"\n",
    "        Take audio chunks from the out_queue and send them via the Gemini session.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            msg = await self.out_queue.get()\n",
    "            await self.session.send(input=msg)\n",
    "\n",
    "    async def receive_audio(self):\n",
    "        \"\"\"\n",
    "        Receive audio from the Gemini session, then place raw PCM data from responses into audio_in_queue.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # 'session.receive()' returns an async generator.\n",
    "            turn = self.session.receive()\n",
    "            async for response in turn:\n",
    "                if response.data:\n",
    "                    self.audio_in_queue.put_nowait(response.data)\n",
    "                if response.text:\n",
    "                    print(response.text, end=\"\")\n",
    "            # Clear out any unread audio chunks if the turn completes.\n",
    "            while not self.audio_in_queue.empty():\n",
    "                self.audio_in_queue.get_nowait()\n",
    "\n",
    "    async def play_audio(self):\n",
    "        \"\"\"\n",
    "        Take PCM audio chunks from the incoming audio queue and play them through the speaker.\n",
    "        \"\"\"\n",
    "        output_stream = await asyncio.to_thread(\n",
    "            pya.open,\n",
    "            format=FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=RECEIVE_SAMPLE_RATE,\n",
    "            output=True,\n",
    "        )\n",
    "        while True:\n",
    "            audio_chunk = await self.audio_in_queue.get()\n",
    "            await asyncio.to_thread(output_stream.write, audio_chunk)\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        Connect to the Gemini Live API and run the audio pipeline tasks concurrently.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with client.aio.live.connect(model=MODEL, config=CONFIG) as session, asyncio.TaskGroup() as tg:\n",
    "                self.session = session\n",
    "                # Start asynchronous tasks for capturing, sending, receiving, and playing audio.\n",
    "                tg.create_task(self.listen_audio())\n",
    "                tg.create_task(self.send_audio())\n",
    "                tg.create_task(self.receive_audio())\n",
    "                tg.create_task(self.play_audio())\n",
    "                # Here, we simply run indefinitely. In your notebook, you may stop the execution with an interrupt.\n",
    "                await asyncio.sleep(3600)\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"Audio loop was cancelled.\")\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(e)\n",
    "        finally:\n",
    "            if self.audio_stream:\n",
    "                self.audio_stream.close()\n",
    "\n",
    "# For running in a notebook or script.\n",
    "async def main():\n",
    "    audio_loop = AudioLoop()\n",
    "    await audio_loop.run()\n",
    "\n",
    "# If running as a script, use asyncio.run(main())\n",
    "if __name__ == \"__main__\":\n",
    "    await main"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-13T18:09:10.471119592Z",
     "start_time": "2025-04-13T18:08:48.188518835Z"
    }
   },
   "id": "afa6b54d8a73324a"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import io\n",
    "from google.genai import types\n",
    "import base64\n",
    "from typing import Optional, List, Dict\n",
    "import json\n",
    "from PIL import Image\n",
    "from google import genai\n",
    "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
    "\n",
    "\n",
    "\n",
    "class ImageGenerator:\n",
    "\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.google_search_tool = Tool(\n",
    "            google_search=GoogleSearch()\n",
    "        )\n",
    "    def _call_text_model(self, prompt: str) -> str:\n",
    "\n",
    "        \n",
    "        response = self.client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "            'response_mime_type': 'application/json' \n",
    "        }\n",
    "        )\n",
    "\n",
    "      \n",
    "        return response.text\n",
    "\n",
    "    def refine_prompt(self, base_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses the search tool to polish a base prompt into a more professional style.\n",
    "\n",
    "        :param base_prompt: Initial userâ€supplied prompt.\n",
    "        :return: Refined prompt.\n",
    "        \"\"\"\n",
    "        response = self.client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=base_prompt,\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[self.google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            ))\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            return part.text.strip()\n",
    "\n",
    "    def recommend_trends(self,\n",
    "                         gender: str,\n",
    "                         dress_type: str,\n",
    "                         occasion: str,\n",
    "                         region: str = \"Pakistan\"\n",
    "                         ) -> Dict[str, List[str]]:\n",
    "       \n",
    "        base_prompt = f\"\"\"Search the List the top {gender} {dress_type} design trends \n",
    "            for a {occasion} in {region}, with professional detail.\"\"\"\n",
    "        \n",
    "        polished = self.refine_prompt(base_prompt)\n",
    "\n",
    "        # Build JSONâ€output prompt\n",
    "        json_prompt =  f\"\"\"You are a fashion industry expert. You will extract the trends description with proper dress type {dress_type} and occasion {occasion} for the gender {gender}. You will extract the one trend design of dress and mention the color and design on one design only in provided description and first you will analyze text and check how many design of dress it contain for example it conatin one then return one trend descrption in response in case 2 design then return 2 trends in list You will define and mention proper dress design and colors everything in trend description. You will extract the detailed trend descriptions of design in list form.\n",
    "            Generate a JSON output structure according to following format:\n",
    "            \n",
    "            {{'trends': [write here with proper dress type, color , detail design in string in the form of list for how many design it detect in following context]}}\n",
    "            Context:\n",
    "            {polished}\"\"\"\n",
    "        \n",
    "\n",
    "        raw_json = self._call_text_model(prompt=json_prompt)\n",
    "      \n",
    "        \n",
    "        data = json.loads(raw_json)\n",
    "        return {'trends': data['trends'], \n",
    "                'gender': gender,\n",
    "                'occasion': occasion}\n",
    "\n",
    "\n",
    "    def generate_image(self, trend_description: str, gender: str, occasion:str) -> None:\n",
    "  \n",
    "        prompt = f\"\"\"As an expert illustrator, create a professional-quality image for this design. I need the full quality image and fullfill the all following requirements. here is concept: gender : {gender} , occasion : {occasion}, {trend_description}\"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        # Use the refined prompt to generate an image with attributes emphasizing 'professional' and 'best'\n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_modalities=[\"Text\", \"Image\"]\n",
    "            ),\n",
    "        )\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            # Skip parts with text content.\n",
    "            if part.text is not None:\n",
    "                continue\n",
    "            elif part.inline_data is not None:\n",
    "                mime = part.inline_data.mime_type\n",
    "                data = part.inline_data.data\n",
    "                # If data is a base64 string, decode it.\n",
    "                if isinstance(data, str):\n",
    "                    data = base64.b64decode(data)\n",
    "                return data\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:39.939422356Z",
     "start_time": "2025-04-23T15:10:39.895598295Z"
    }
   },
   "id": "945dc4fe8b33f8ec"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "ig = ImageGenerator(api_key = 'AIzaSyAAaqM_Vamz26ZlsI3sgkYvcMQcGcCI3To')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:40.231811062Z",
     "start_time": "2025-04-23T15:10:40.052891746Z"
    }
   },
   "id": "a18586cdcb1c3a23"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "answer = ig.recommend_trends('male',\n",
    "                         'kurta shalwar',\n",
    "                         'eid ul fitr')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:48.386414448Z",
     "start_time": "2025-04-23T15:10:40.315344841Z"
    }
   },
   "id": "44c60f7c8aadabe8"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightening Kurtas: Delicate metallic threads and a soft sheen are used in these kurtas. Suitable for daytime and evening wear. Color can be varied depend on user choice and design is simple and elegant.\n"
     ]
    }
   ],
   "source": [
    "print(answer['trends'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:48.447387867Z",
     "start_time": "2025-04-23T15:10:48.427003830Z"
    }
   },
   "id": "24709512b144804b"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to professional_best_design.png\n"
     ]
    }
   ],
   "source": [
    "ig.generate_image(answer['trends'][0], answer['gender'], answer['occasion'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:11:37.979051761Z",
     "start_time": "2025-04-23T15:11:28.474552597Z"
    }
   },
   "id": "dfa1a6cbefd6c879"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from aitrendshopper.database import chats_collection\n",
    "from aitrendshopper.config import settings\n",
    "\n",
    "FAISS_DIR     = \"./faiss_index_dir\"\n",
    "GEMINI_API_KEY= os.getenv(\"GOOGLE_API_KEY\", settings.gemini_api)\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "\n",
    "\n",
    "# --- Chatbot wrapper ---\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        # Gemini client + search tool\n",
    "        self.client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        self.google_search_tool = Tool(google_search=GoogleSearch())\n",
    "        # Embeddings + FAISS\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "        if os.path.isdir(FAISS_DIR):\n",
    "            self.faiss = FAISS.load_local(FAISS_DIR, self.embeddings,\n",
    "                                            allow_dangerous_deserialization=True\n",
    "            )\n",
    "        else:\n",
    "            # initialize empty index\n",
    "            self.faiss = FAISS.from_documents([], self.embeddings)\n",
    "            self.faiss.save_local(FAISS_DIR)\n",
    "\n",
    "        # Chat LLM\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-pro\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "\n",
    "    async def create_chat(self, user_id: str):\n",
    "        await chats_collection.insert_one({\"_id\": user_id, \"history\": []})\n",
    "\n",
    "    async def load_chat(self, user_id: str) -> List[dict]:\n",
    "        rec = await chats_collection.find_one({\"_id\": user_id})\n",
    "        return rec[\"history\"] if rec else []\n",
    "\n",
    "    async def update_chat(self, user_id: str, question: str, answer: str):\n",
    "        await chats_collection.update_one(\n",
    "            {\"_id\": user_id},\n",
    "            {\"$push\": {\"history\": {\"question\": question, \"response\": answer}}}\n",
    "        )\n",
    "\n",
    "    async def update_faiss(self, plain_text: str) -> dict:\n",
    "        # 1. Wrap & split\n",
    "        docs   = [Document(page_content=plain_text)]\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        new_faiss_index = FAISS.from_documents(chunks, self.embeddings)\n",
    "\n",
    "        self.faiss.merge_from(new_faiss_index)\n",
    "\n",
    "        self.faiss.save_local(FAISS_DIR)\n",
    "        return {\"status\": True, \"message\": \"FAISS index updated with new text.\"}\n",
    "\n",
    "    async def inference(self, question: str, chat_history: str, search = False) -> str:\n",
    "        # 1) retrieval\n",
    "        docs    = self.faiss.similarity_search(question, k=5)\n",
    "        context = \" \".join(d.page_content for d in docs)\n",
    "        print(context)\n",
    "        # 2) build prompt\n",
    "        prompt = (\n",
    "            f\"You are expert shop-advisor aitrendshopper.\\n\"\n",
    "            f\"Context: {context}\\n\"\n",
    "            f\"Chat history: {chat_history}\\n\"\n",
    "            f\"User: {question}\\n\"\n",
    "            \"Assistant:\"\n",
    "        )\n",
    "        if search:\n",
    "            resp = self.client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=prompt,\n",
    "                config=GenerateContentConfig(\n",
    "                    tools=[self.google_search_tool],\n",
    "                    response_modalities=[\"TEXT\"],\n",
    "                )\n",
    "            )\n",
    "            resp_answer = ''\n",
    "            for part in resp.candidates[0].content.parts:\n",
    "                resp_answer = resp_answer + ' ' + part.text.strip()\n",
    "            return resp_answer\n",
    "        else:\n",
    "            response = self.client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "         \n",
    "        )\n",
    "\n",
    "      \n",
    "            return response.text\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "bot = Chatbot()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:31:38.597875766Z",
     "start_time": "2025-04-25T07:31:38.366542675Z"
    }
   },
   "id": "238f98086d1990f3"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is today trend Your very long plain text goes here...\n"
     ]
    }
   ],
   "source": [
    "answer = await bot.inference('what is today trend', '', True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:31:50.013410301Z",
     "start_time": "2025-04-25T07:31:38.709225550Z"
    }
   },
   "id": "6ba6137df59f6105"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To give you the most relevant information on today's trends, I need a little more information about what kind of trends you're interested in! Could you tell me what area you'd like to know about? For example, are you interested in:\n",
      "\n",
      "*   **Fashion trends?**\n",
      "*   **Beauty trends?**\n",
      "*   **Technology trends?**\n",
      "*   **Home decor trends?**\n",
      "*   **Social media trends?**\n",
      "*   **Food trends?**\n",
      "\n",
      "Once I know what kind of trends you're looking for, I can give you a much more specific and helpful answer. Okay, based on my research from today, April 25, 2025, here's a rundown of potential trends in various sectors:\n",
      "\n",
      "**Beauty:**\n",
      "\n",
      "*   **Continued Emphasis on Natural and Organic Ingredients:** Consumers are still very interested in natural and organic ingredients in their beauty products.\n",
      "*   **Hyper-Personalization:** Expect to see more AI and AR being used to personalize beauty product recommendations and experiences.\n",
      "*   **Biotech Innovations:** The beauty industry is increasingly using biotechnology to find sustainable beauty ingredients.\n",
      "*   **Men's Beauty Products:** The demand for male grooming products is on the rise.\n",
      "*   **Diversity and Inclusion:** A continued call for more diversity and inclusion in the beauty industry.\n",
      "*   **Tech-Enhanced Beauty:** High-tech wands using LED light therapy for skincare are gaining popularity, offering at-home versions of professional treatments.\n",
      "*   **Ingredient-Focused Shopping:** Consumers are paying more attention to the ingredients in their skincare and beauty products than brand names.\n",
      "*   **Korean Beauty:** Korean beauty trends continue to be influential.\n",
      "\n",
      "**Fashion:**\n",
      "\n",
      "*   **Custom-Made Clothing (Slow Fashion):** Consumers are increasingly interested in custom-made clothing.\n",
      "*   **Bold Colors:** Fresh and bold designs are in, driven by a desire to come back to life after lockdowns.\n",
      "*   **Video is Still King:** Short-form video continues to dominate, but long-form video is resurging as well.\n",
      "*   **Cultural Fluency:** Cultural fluency is a must for brands.\n",
      "*   **Strong Brand Storytelling:** Strong brand storytelling and engagement are audience must-haves.\n",
      "\n",
      "**Food:**\n",
      "\n",
      "*   **Cocktails and Alcohol-Inspired Flavors:** Alcoholic drinks are inspiring new desserts and flavors.\n",
      "*   **\"Swicy\" Flavors:** Sweet and spicy flavor combinations are popular.\n",
      "*   **Texture Mash-Ups:** Combining contrasting textures in one bite (e.g., a croissant with cookie dough).\n",
      "*   **Plant-Based Options:** Continued growth in demand for plant-based alternatives.\n",
      "*   **Global Flavors and Fusion Cuisine:** Blending ingredients and techniques from around the world.\n",
      "*   **Functional Foods:** Foods with positive effects on gut health and overall well-being are gaining popularity.\n",
      "*   **Local Abundance:** Celebrating local produce.\n",
      "*   **Minimal Waste Menus:** Maximizing resources and reducing environmental impact.\n",
      "*   **Ingredients and Beyond:** Consumers are increasingly seeking value from high quality ingredients.\n",
      "\n",
      "**Home Decor:**\n",
      "\n",
      "*   **Bold Colors:** Bold colors are taking center stage.\n",
      "*   **Sustainable Materials:** Sustainable materials are key.\n",
      "*   **Multifunctional Furniture:** Furniture that serves multiple purposes is trending.\n",
      "*   **Minimalist with a Cozy Twist:** Combining Scandinavian design with cozy elements.\n",
      "*   **Textured Walls & Ceilings:** Adding texture to walls.\n",
      "*   **Comfortable Furniture:** Casually comfortable furniture is embraced.\n",
      "*   **Warm Metallics:** Gold, bronze, and copper tones are popular.\n",
      "*   **Nature Indoors:** Natural wood and stones are being incorporated.\n",
      "*   **Warm Colors:** Warm colors are making a comeback.\n",
      "*   **Curves:** Curves are replacing straight lines.\n",
      "*   **Dark Wood:** Darker woods are making a comeback.\n",
      "*   **Dark, Earthy Shades:** Dark, earthy shades are trending.\n",
      "\n",
      "**Social Media:**\n",
      "\n",
      "*   **Short-Form Video:** Short-form video remains dominant.\n",
      "*   **Authenticity and UGC:** Demand for authenticity and user-generated content is high.\n",
      "*   **Social Commerce:** Social commerce continues to rise.\n",
      "*   **Private Communities:** Prioritizing connections over follower counts.\n",
      "*   **AI-Generated Content:** The rise of AI-generated content.\n",
      "*   **Content Experimentation:** Social teams are ditching brand consistency to push creative boundaries.\n",
      "*   **Social Performance:** Listening launches social pros into their performance marketing era.\n",
      "*   **AI Content:** Generative AI is officially on the team.\n",
      "*   **Video is Still King:** Video content is a priority across networks.\n",
      "*   **Social Search:** Social search is a top priority.\n",
      "\n",
      "**Technology:**\n",
      "\n",
      "*   **Artificial Intelligence (AI) and Machine Learning (ML):** AI continues to be a dominant trend.\n",
      "*   **5G:** Expansion of 5G networks.\n",
      "*   **Internet of Things (IoT):** Enterprise and public use of IoT is expanding.\n",
      "*   **Blockchain Technology:** Blockchain technology continues to evolve.\n",
      "*   **Augmented Reality (AR) and Virtual Reality (VR):** AR and VR are becoming more integrated.\n",
      "*   **Quantum Computing:** Quantum computing is moving towards real-world application.\n",
      "*   **Edge Computing:** Edge computing is transforming how enterprises use data.\n",
      "*   **Robotic Process Automation (RPA):** Adoption of robotic process automation continues to grow.\n",
      "*   **Democratization of AI:** AI is becoming more accessible.\n",
      "\n",
      "To give you even *more* specific information, tell me which of these areas is most interesting to you!\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:31:50.878061494Z",
     "start_time": "2025-04-25T07:31:50.851724042Z"
    }
   },
   "id": "b660592ba9248553"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# --- Endpoints ---\n",
    "@app.post(\"/get_response\", response_model=ResponsePayload)\n",
    "async def get_response(data: ChatRequest):\n",
    "    user = await users_collection.find_one({\"token\": data.token})\n",
    "    if not user:\n",
    "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
    "\n",
    "    uid = str(user[\"_id\"])\n",
    "    if not await bot.load_chat(uid):\n",
    "        await bot.create_chat(uid)\n",
    "\n",
    "    history = await bot.load_chat(uid)\n",
    "    answer  = await bot.inference(data.query, history)\n",
    "    await bot.update_chat(uid, data.query, answer)\n",
    "    return ResponsePayload(status=True, answer=answer)\n",
    "\n",
    "\n",
    "@app.post(\"/get_history\", response_model=HistoryResponse)\n",
    "async def get_history(data: HistoryRequest):\n",
    "    user = await users_collection.find_one({\"token\": data.token})\n",
    "    if not user:\n",
    "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
    "\n",
    "    history = await bot.load_chat(str(user[\"_id\"]))\n",
    "    return HistoryResponse(history=history)\n",
    "\n",
    "\n",
    "@app.post(\"/update_faiss\", response_model=ResponsePayload)\n",
    "async def update_faiss_endpoint(data: UpdateFaissRequest):\n",
    "    user = await users_collection.find_one({\"token\": data.token})\n",
    "    if not user:\n",
    "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
    "\n",
    "    result = await bot.update_faiss(data.text)\n",
    "    return ResponsePayload(**result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:15:58.538852966Z",
     "start_time": "2025-04-25T07:15:57.324141545Z"
    }
   },
   "id": "1cd1954916626c41"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-25T06:40:50.533405226Z",
     "start_time": "2025-04-25T06:40:50.509480170Z"
    }
   },
   "id": "76b48af55fab4979"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'GoogleSearch' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maitrendshopper\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservices\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchathandler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Chatbot\n\u001B[1;32m      3\u001B[0m search_tool \u001B[38;5;241m=\u001B[39m Chatbot()\u001B[38;5;241m.\u001B[39mgoogle_search_tool  \n\u001B[0;32m----> 5\u001B[0m search_results \u001B[38;5;241m=\u001B[39m \u001B[43msearch_tool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgoogle_search\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpents\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_results\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'GoogleSearch' object is not callable"
     ]
    }
   ],
   "source": [
    "from aitrendshopper.services.chathandler import Chatbot\n",
    "\n",
    "search_tool = Chatbot().google_search_tool  \n",
    "\n",
    "search_results = search_tool.google_search('pents', num_results=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T19:49:21.492921596Z",
     "start_time": "2025-04-28T19:49:15.574534238Z"
    }
   },
   "id": "3b1643e49828eca3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "import base64\n",
    "from typing import Optional, List, Dict\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai.types import Tool, GenerateContentConfig, GoogleSearch\n",
    "\n",
    "class RecommendGenerator:\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "\n",
    "        self.api_key = api_key\n",
    "\n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.google_search_tool = Tool(\n",
    "            google_search=GoogleSearch()\n",
    "        )\n",
    "\n",
    "    def _call_text_model(self, prompt: str) -> str:\n",
    "\n",
    "        response = self.client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'response_mime_type': 'application/json'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    def refine_prompt(self, base_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses the search tool to polish a base prompt into a more professional style.\n",
    "\n",
    "        :param base_prompt: Initial userâ€supplied prompt.\n",
    "        :return: Refined prompt.\n",
    "        \"\"\"\n",
    "        response = self.client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=base_prompt,\n",
    "            config=GenerateContentConfig(\n",
    "                tools=[self.google_search_tool],\n",
    "                response_modalities=[\"TEXT\"],\n",
    "            ))\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            return part.text.strip()\n",
    "\n",
    "    def recommend_trends(self\n",
    "                         ):\n",
    "\n",
    "        base_prompt = f\"\"\"Search the top trend in dress name which kind of dress likes in pakistan pent, shalwar, kameez etc everything detail search out for men, boys , girl, females for everyone\"\"\"\n",
    "\n",
    "        polished = self.refine_prompt(base_prompt)\n",
    "\n",
    "        # Build JSONâ€output prompt\n",
    "        json_prompt = f\"\"\"You are a fashion industry expert. You will extract the fress name from the following text top 6 dres name\n",
    "            Generate a JSON output structure according to following format:\n",
    "\n",
    "            {{'dresses': [write here dresses here ]}}\n",
    "            Context:\n",
    "            {polished}\"\"\"\n",
    "\n",
    "        raw_json = self._call_text_model(prompt=json_prompt)\n",
    "\n",
    "        data = json.loads(raw_json)\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T19:59:31.222467597Z",
     "start_time": "2025-04-28T19:59:31.191744930Z"
    }
   },
   "id": "719d4c134127c832"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from aitrendshopper.config import settings\n",
    "rg = RecommendGenerator(api_key=settings.gemini_api)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T19:59:32.099486170Z",
     "start_time": "2025-04-28T19:59:31.992639672Z"
    }
   },
   "id": "6fa56314a9839c86"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "answers = rg.recommend_trends()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T20:08:32.356273253Z",
     "start_time": "2025-04-28T20:08:21.523436403Z"
    }
   },
   "id": "bf383b7146c53eab"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for text 1: 3072\n",
      "Embedding for text 2: 3072\n",
      "Embedding for text 3: 3072\n",
      "Embedding for text 4: 3072\n",
      "Embedding for text 5: 3072\n",
      "Embedding for text 6: 3072\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-28T20:08:33.958468606Z",
     "start_time": "2025-04-28T20:08:32.362919470Z"
    }
   },
   "id": "62b4c8603ebed6cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "62a5d7cc4c8c2df6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
